options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 3
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
# Chunk 4
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
# Chunk 5
#Preprocessing - train
train$median_relevance <- factor(train$median_relevance)
train_query <- cleanCorpus(train$query)
train_product_title <- cleanCorpus(train$product_title)
train_product_description <- cleanCorpus(train$product_description)
#Preprocessing - test
test_query <- cleanCorpus(test$query)
test_product_title <- cleanCorpus(test$product_title)
test_product_description <- cleanCorpus(test$product_description)
# Chunk 6
#create a document term matrix - train
DTM_query_of_train <- DocumentTermMatrix(train_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_train <- DocumentTermMatrix(train_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
DTM_product_description_of_train <- DocumentTermMatrix(train_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
#create a document term matrix - test
DTM_query_of_test <- DocumentTermMatrix(test_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_test <- DocumentTermMatrix(test_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
DTM_product_description_of_test <- DocumentTermMatrix(test_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
# Chunk 7
#sum of two matrices - train
q_t_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_title_of_train)
q_d_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_product_description_of_train)
#sum of two matrices - test
q_t_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_title_of_test)
q_d_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_product_description_of_test)
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 3
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
# Chunk 4
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
# Chunk 5
#Preprocessing - train
train$median_relevance <- factor(train$median_relevance)
train_query <- cleanCorpus(train$query)
train_product_title <- cleanCorpus(train$product_title)
train_product_description <- cleanCorpus(train$product_description)
#Preprocessing - test
test_query <- cleanCorpus(test$query)
test_product_title <- cleanCorpus(test$product_title)
test_product_description <- cleanCorpus(test$product_description)
# Chunk 6
#create a document term matrix - train
DTM_query_of_train <- DocumentTermMatrix(train_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_train <- DocumentTermMatrix(train_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
DTM_product_description_of_train <- DocumentTermMatrix(train_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
#create a document term matrix - test
DTM_query_of_test <- DocumentTermMatrix(test_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_test <- DocumentTermMatrix(test_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
DTM_product_description_of_test <- DocumentTermMatrix(test_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
# Chunk 7
#sum of two matrices - train
q_t_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_title_of_train)
q_d_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_product_description_of_train)
#sum of two matrices - test
q_t_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_title_of_test)
q_d_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_product_description_of_test)
# Chunk 8
#count sum of two matrices - train
q_t_train_one<-c()
q_t_train_two<-c()
q_d_train_one<-c()
q_d_train_two<-c()
for(i in seq(train$id)){
q_t_train_one[i]<-sum(q_t_train[i,] == 1)
q_t_train_two[i]<-sum(q_t_train[i,] >= 2)
q_d_train_one[i]<-sum(q_d_train[i,] == 1)
q_d_train_two[i]<-sum(q_d_train[i,] >= 2)
}
##count sum of two matrices - test
q_t_test_one<-c()
q_t_test_two<-c()
q_d_test_one<-c()
q_d_test_two<-c()
for(i in seq(test$id)){
q_t_test_one[i]<-sum(q_t_test[i,] == 1)
q_t_test_two[i]<-sum(q_t_test[i,] >= 2)
q_d_test_one[i]<-sum(q_d_test[i,] == 1)
q_d_test_two[i]<-sum(q_d_test[i,] >= 2)
}
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 3
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
# Chunk 4
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
# Chunk 5
#Preprocessing - train
train$median_relevance <- factor(train$median_relevance)
train_query <- cleanCorpus(train$query)
train_product_title <- cleanCorpus(train$product_title)
train_product_description <- cleanCorpus(train$product_description)
#Preprocessing - test
test_query <- cleanCorpus(test$query)
test_product_title <- cleanCorpus(test$product_title)
test_product_description <- cleanCorpus(test$product_description)
# Chunk 6
#create a document term matrix - train
DTM_query_of_train <- DocumentTermMatrix(train_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_train <- DocumentTermMatrix(train_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
DTM_product_description_of_train <- DocumentTermMatrix(train_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
#create a document term matrix - test
DTM_query_of_test <- DocumentTermMatrix(test_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_test <- DocumentTermMatrix(test_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
DTM_product_description_of_test <- DocumentTermMatrix(test_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
# Chunk 7
#sum of two matrices - train
q_t_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_title_of_train)
q_d_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_product_description_of_train)
#sum of two matrices - test
q_t_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_title_of_test)
q_d_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_product_description_of_test)
# Chunk 8
#count sum of two matrices - train
q_t_train_one<-c()
q_t_train_two<-c()
q_d_train_one<-c()
q_d_train_two<-c()
for(i in seq(train$id)){
q_t_train_one[i]<-sum(q_t_train[i,] == 1)
q_t_train_two[i]<-sum(q_t_train[i,] >= 2)
q_d_train_one[i]<-sum(q_d_train[i,] == 1)
q_d_train_two[i]<-sum(q_d_train[i,] >= 2)
}
##count sum of two matrices - test
q_t_test_one<-c()
q_t_test_two<-c()
q_d_test_one<-c()
q_d_test_two<-c()
for(i in seq(test$id)){
q_t_test_one[i]<-sum(q_t_test[i,] == 1)
q_t_test_two[i]<-sum(q_t_test[i,] >= 2)
q_d_test_one[i]<-sum(q_d_test[i,] == 1)
q_d_test_two[i]<-sum(q_d_test[i,] >= 2)
}
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 3
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
# Chunk 4
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
# Chunk 5
#Preprocessing - train
train$median_relevance <- factor(train$median_relevance)
train_query <- cleanCorpus(train$query)
train_product_title <- cleanCorpus(train$product_title)
train_product_description <- cleanCorpus(train$product_description)
#Preprocessing - test
test_query <- cleanCorpus(test$query)
test_product_title <- cleanCorpus(test$product_title)
test_product_description <- cleanCorpus(test$product_description)
# Chunk 6
#create a document term matrix - train
DTM_query_of_train <- DocumentTermMatrix(train_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_train <- DocumentTermMatrix(train_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
DTM_product_description_of_train <- DocumentTermMatrix(train_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_train)))
#create a document term matrix - test
DTM_query_of_test <- DocumentTermMatrix(test_query,
control = list(wordLengths=c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = NULL))
DTM_title_of_test <- DocumentTermMatrix(test_product_title,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
DTM_product_description_of_test <- DocumentTermMatrix(test_product_description,
control = list(wordLengths = c(2,Inf),
bounds = list(global = c(0, Inf)),
dictionary = Terms(DTM_query_of_test)))
# Chunk 7
#sum of two matrices - train
q_t_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_title_of_train)
q_d_train<-as.matrix(DTM_query_of_train) + as.matrix(DTM_product_description_of_train)
#sum of two matrices - test
q_t_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_title_of_test)
q_d_test<-as.matrix(DTM_query_of_test) + as.matrix(DTM_product_description_of_test)
# Chunk 8
#count sum of two matrices - train
q_t_train_one<-c()
q_t_train_two<-c()
q_d_train_one<-c()
q_d_train_two<-c()
for(i in seq(train$id)){
q_t_train_one[i]<-sum(q_t_train[i,] == 1)
q_t_train_two[i]<-sum(q_t_train[i,] >= 2)
q_d_train_one[i]<-sum(q_d_train[i,] == 1)
q_d_train_two[i]<-sum(q_d_train[i,] >= 2)
}
##count sum of two matrices - test
q_t_test_one<-c()
q_t_test_two<-c()
q_d_test_one<-c()
q_d_test_two<-c()
for(i in seq(test$id)){
q_t_test_one[i]<-sum(q_t_test[i,] == 1)
q_t_test_two[i]<-sum(q_t_test[i,] >= 2)
q_d_test_one[i]<-sum(q_d_test[i,] == 1)
q_d_test_two[i]<-sum(q_d_test[i,] >= 2)
}
# Chunk 9
#calculate the new score - train
for(i in seq(train$id)){
train$score[i]<-(q_t_train_one[i]+q_d_train_one[i])/(q_t_train_two[i]+q_d_train_two[i]+1)
}
#calculate the new score - test
for(i in seq(test$id)){
test$score[i]<-(q_t_test_one[i]+q_d_test_one[i])/(q_t_test_two[i]+q_d_test_two[i]+1)
}
# Chunk 1: set-options
options(width = 200)
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
options(width = 200)
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
setwd("C:/Users/Bareket/Dropbox/works/Ex4")
# Chunk 1: set-options
options(width = 200)
# Chunk 2
library(tm)
library(SnowballC)
library(readr)
library(randomForest)
library(party)
library(caret)
# Chunk 3
cleanCorpus <- function(toClean){
news_corpus <- VectorSource(toClean)
docs <- Corpus(news_corpus)
docs <- tm_map(docs,removePunctuation)
for(j in seq(docs)){
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
#stemSentence function - reducing inflected (or sometimes derived) words to their word stem, base or root           form-generally a written word form
docs[[j]]<- strsplit(docs[[j]], "[[:blank:]]")[[1]]
docs[[j]]<- wordStem(docs[[j]], "english")
paste(docs[[j]], collapse=" ")
}
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stemDocument)                #/delete
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
return(docs)
}
# Chunk 4
#Loading the data - train
unzip("train.csv.zip")
train <- read_csv("train.csv")
#Loading the data - test
unzip("test.csv.zip")
test  <- read_csv("test.csv")
# Chunk 5
#Preprocessing - train
train$median_relevance <- factor(train$median_relevance)
train_query <- cleanCorpus(train$query)
train_product_title <- cleanCorpus(train$product_title)
train_product_description <- cleanCorpus(train$product_description)
#Preprocessing - test
test_query <- cleanCorpus(test$query)
